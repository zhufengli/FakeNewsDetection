{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import _pickle as cPickle \n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = pd.read_csv('data/train_bodies.csv')\n",
    "stances = pd.read_csv('data/train_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies[bodies['Body ID']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies['Body ID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stances[stances['Body ID']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using texts and lables\n",
    "# trainDF = pandas.DataFrame()\n",
    "# trainDF['text'] = dataset['summaries']\n",
    "# trainDF['label'] = dataset['departs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the dataset into training and validation datasets \n",
    "# train_x, test_x, train_y, test_y = model_selection.train_test_split(trainDF.drop([27670])['text'],trainDF.drop([27670])['label'],stratify=trainDF.drop([27670])['label'],shuffle=True,random_state=1410)\n",
    "# # label encode the target variable \n",
    "# encoder = preprocessing.LabelEncoder()\n",
    "# train_y = encoder.fit_transform(train_y)\n",
    "# test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -O ./data/embedding https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o ./data/embedding -d ./data/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n",
    "#Embedding the words\n",
    "def load_vectors(fname,nb):\n",
    "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    count = 0\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = numpy.asarray(tokens[1:], dtype='float32')\n",
    "        count += 1\n",
    "        if count > nb:\n",
    "            break\n",
    "    return data\n",
    "\n",
    "#Only load the most frequent words 150k\n",
    "embeddings_index = load_vectors('data/wiki-news-300d-1M.vec',150000) \n",
    "\n",
    "# # create a tokenizer \n",
    "# token = text.Tokenizer()\n",
    "# token.fit_on_texts(trainDF['text'])\n",
    "# word_index = token.word_index\n",
    "\n",
    "# # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "# test_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x), maxlen=70)\n",
    "\n",
    "# # create token-embedding mapping\n",
    "# embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
